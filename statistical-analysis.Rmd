---
title: "COVID Statistical Analysis"
author: "Hongyin Lai, Yuan Li, Tara Prezioso, Alison Rector, Jeffrey Brennan, Swaminathan Kumar, Jose-Miguel Yamal"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
set.seed(1)
```

```{r library calls, warning=FALSE, message=FALSE}
# Data manipulation & cleaning
library(tidyverse)
library(reshape2)
library(nlme)        # gapply
library(lubridate)   # date cleaning

# Viz
library(gridExtra)
library(ggmap)
library(maps)
library(mapdata)

# Modeling & stats
library(mgcv)       # gam
library(R0)
library(Kendall)    # Mann-Kendall

# Time series & forecasting
library(forecast)
library(zoo)
library(astsa)
library(fpp2)
```

# TODO

- combine time series predictions into one generalized function & improve filenames/paths

# RT ANALYSIS

```{r Read in data}
#read in full DSHS data with county and TSA information
# TOOD: rename
full.dshs <- read.csv("combined-datasets/county.csv")
tsa.dshs <- read.csv("combined-datasets/tsa.csv")
phr.df <- read.csv("combined-datasets/phr.csv")

date_out = as.Date(ifelse((Sys.time() < as.POSIXct(paste0(Sys.Date(), '16:00'), tz = 'America/Chicago')),
                   Sys.Date() - 1,
                   Sys.Date()))
```

```{r rt data clean}
rt.data.clean<-function(covid.data) {
  
  # select relevant metadata & DSHS population estimates
  rt.cols <- c("County", "Date", "TSA_Name", "TSA", "PHR", "PHR_Name",
               "Metro_Area", "Cases_Daily_New", "Population_DSHS")
  
  covid.data <- covid.data[, names(covid.data) %in% rt.cols]
             
  # set correct types
  covid.data$Date <- as.Date(covid.data$Date)
  covid.data$Cases_Daily_New <- as.numeric(covid.data$Cases_Daily_New)
  
  # TODO: decide on how to handle drops in cumulative counts -> negative daily cases
  covid.data <- covid.data %>% 
    mutate(Cases_Daily_New = ifelse(Cases_Daily_New < 0, 0, Cases_Daily_New))
  
  return(covid.data)
}

```


```{r rt separate dataframe}
#separate county, metro, TSA and state data into separate dataframes
rt.data.organize <- function(mycounty, mytsa, myphr){
  
  ### COUNTY ###
  cleaned.county <- rt.data.clean(mycounty)
  county.df <- cleaned.county%>%dplyr::select(-TSA_Name, -Metro_Area, -TSA)
  
  ### TSA ###
  TSA.df <- rt.data.clean(mytsa)

  ### PHR ###
  PHR.df <- rt.data.clean(myphr)
  ### METRO ###
  # calculate daily cases and population at metro level, drop repeated rows
  metro.temp <- cleaned.county %>%
    group_by(Metro_Area,Date) %>%
    mutate(metro_Cases_Daily_New=sum(Cases_Daily_New, na.rm=TRUE)) %>%
    mutate(metro_pop_DSHS=sum(Population_DSHS, na.rm=TRUE)) %>%
    dplyr::select(Date, Metro_Area, metro_Cases_Daily_New, metro_pop_DSHS) %>%
    distinct()
  
  
  metro.df <- data.frame(Date = metro.temp$Date,
                         Metro_Area = metro.temp$Metro_Area,
                         Cases_Daily_New = metro.temp$metro_Cases_Daily_New,
                         Population_DSHS = metro.temp$metro_pop_DSHS)
  
  
  ### STATE ###
  # calculate daily cases and population at state level, drop repeated rows
  state.temp <- TSA.df %>%
    group_by(Date) %>%
    mutate(state_Cases_Daily_New=sum(Cases_Daily_New, na.rm=TRUE)) %>%
    mutate(state_pop_DSHS=sum(Population_DSHS, na.rm=TRUE)) %>%
    dplyr::select(Date, state_Cases_Daily_New, state_pop_DSHS) %>%
    distinct()
  
  
  state.df <- data.frame(Date = state.temp$Date, 
                         Cases_Daily_New = state.temp$state_Cases_Daily_New,
                         Population_DSHS = state.temp$state_pop_DSHS)

  ### OUTPUT ###
  rt.df.out <- list(county=county.df, TSA=TSA.df, state=state.df, metro=metro.df, phr = PHR.df)
  return(rt.df.out)
}

```

```{r rt extraction}
rt.df.extraction <- function(Rt.estimate.output) {

  # extract r0 estimate values into dataframe
  rt.df <- setNames(stack(Rt.estimate.output$estimates$TD$R)[2:1], c('Date', 'Rt'))
  rt.df$Date <- as.Date(rt.df$Date)

  # get 95% CI
  CI.lower.list <- Rt.estimate.output$estimates$TD$conf.int$lower
  CI.upper.list <- Rt.estimate.output$estimates$TD$conf.int$upper

  #use unlist function to format as vector
  CI.lower <- unlist(CI.lower.list, recursive = TRUE, use.names = TRUE)
  CI.upper <- unlist(CI.upper.list, recursive = TRUE, use.names = TRUE)

  rt.df$lower <- CI.lower
  rt.df$upper <- CI.upper
  
  rt.df <- rt.df %>%
    mutate(lower = replace(lower, Rt == 0, NA)) %>%
    mutate(upper = replace(upper, Rt == 0, NA)) %>%
    mutate(Rt = replace(Rt, Rt == 0, NA))
  
  return(rt.df)
}
```



```{r rt calculation}
#Function to generate Rt estimates, need to read in data frame and population size
covid.rt <- function(mydata, threshold) {
  
  ### DECLARE VALS ###
  
  #set generation time
  #Tapiwa, Ganyani "Esimating the gen interval for Covid-19":

  # LOGNORMAL OPS
  # gen.time<-generation.time("lognormal", c(4.0, 2.9))
  # gen.time<-generation.time("lognormal", c(4.7,2.9)) #Nishiura

  # GAMMA OPS
  # gen.time<-generation.time("gamma", c(5.2, 1.72)) #Singapore
  # gen.time<-generation.time("gamma", c(3.95, 1.51)) #Tianjin
  gen.time <- generation.time("gamma", c(3.96, 4.75))
  print(as.character(mydata[1,2]))
  
  # TODO: consider removing this and handling na cases directly
  #change na values to 0
  mydata <- mydata %>% replace(is.na(.),0)
  
  # get case average from past month
  recent_case_avg = mydata %>% filter(Date > seq(date_out, length = 2, by = "-1 month")[2]) %>%
    summarize(mean(Cases_Daily_New, na.rm = TRUE)) %>% 
    unlist()
  
  # pop.DSHS <- mydata$Population_DSHS[1]
  
  #Get 7 day moving average of daily cases
  mydata$MA_7day<-rollmean(mydata$Cases_Daily_New, k=7, na.pad=TRUE, align='right')
  #change na values to 0
  mydata<-mydata%>%replace(is.na(.),0)

  #create a vector of new cases 7 day moving average
  mydata.new<-pull(mydata, MA_7day)
  #mydata.new <- pull(mydata, Cases_Daily_New)
  
  # get dates as vectors
  date.vector <- pull(mydata, Date)
  
  #create a named numerical vector using the date.vector as names of new cases
  #Note: this is needed to run R0 package function estimate.R()
  names(mydata.new) <- c(date.vector)
  
  #Find min.date, max.date and row number of max.date
  min.date <- min(mydata$Date)
  max.date <- max(mydata$Date)

  #get row number of March 15 and first nonzero entry
  #NOTE: for 7 day moving average start March 15, for daily start March 9
  #find max row between the two (this will be beginning of rt data used)
  march15.row <- which(mydata$Date=="2020-03-15")
  first.nonzero <- min(which(mydata$Cases_Daily_New>0))
  last.nonzero <- max(which(mydata$Cases_Daily_New>0))
  minrow <- max(march15.row, first.nonzero)

  ### R0 ESTIMATION ###
  # TODO: establish better criteria for # of required daily cases
  # mean daily cases; average last x number of days; % of region pop etc.
  
  if(!is.na(minrow) & recent_case_avg > threshold) {
     
    # declare limits for rt estimation (first nonzero date & last nonzero date)
    i <- minrow
    j <- as.integer(min(last.nonzero, max.date))

    #reduce the vector to be rows from min date (March 9 or first nonzero case) to current date
    mydata.newest <- mydata.new[i:j]
    
    tryCatch({
    rt.DSHS <- estimate.R(mydata.newest, 
                        gen.time, 
                        begin=as.integer(1),
                        end=length(mydata.newest),
                        methods=c("TD"), 
                        pop.size=pop.DSHS,
                        nsim=1000)
  
    rt.DSHS.df <- rt.df.extraction(rt.DSHS)
    },
    error = function(e) {
    cat(as.character(mydata[1,2]))
    rt.DSHS.df <<- data.frame(Date = as.Date(mydata$Date),
                             Rt = rep(NA, length(mydata$Date)),
                             lower = rep(NA, length(mydata$Date)),
                             upper = rep(NA, length(mydata$Date)))
    return(rt.DSHS.df)
    })
  } else {
    # error catch small regions & na values for minrow
    rt.DSHS.df <- data.frame(Date = as.Date(mydata$Date),
                             Rt = rep(NA, length(mydata$Date)),
                             lower = rep(NA, length(mydata$Date)),
                             upper = rep(NA, length(mydata$Date)))
  }
  
  return(rt.DSHS.df)
}
```




```{r prep dataframes, include=FALSE}
covid.frame.list <- rt.data.organize(full.dshs, tsa.dshs, phr.df)

#extract data frames from the list
county.daily <- covid.frame.list$county
TSA.daily <- covid.frame.list$TSA
state.daily <- covid.frame.list$state
metro.daily <- covid.frame.list$metro
phr.daily <- covid.frame.list$phr


case_quant = county.daily %>% filter(Date >= as.Date(date_out - as.difftime(3, unit = 'weeks'))) %>%
  group_by(County) %>%
  mutate(mean_cases = mean(Cases_Daily_New, na.rm = TRUE)) %>%
  dplyr::select(mean_cases) %>%
  ungroup() %>%
  summarize(case_quant = quantile(mean_cases, c(0.4, 0.5, 0.6, 0.7, 0.8), na.rm = TRUE)[4]) %>% 
  unlist()
```

## County

```{r county rt}
#apply the covid.rt function to the county.datily data by county 
county.rt.output <- nlme::gapply(county.daily, FUN = covid.rt,
                                 groups = county.daily$County, threshold = case_quant)

#convert list of dataframes to single dataframe with county as id col
county.rt.df <- data.table::rbindlist(county.rt.output, idcol = TRUE)
colnames(county.rt.df)[1] = 'County'
```

## TMC RT ESTIMATES: NINE COUNTIES
```{r RT ESTIMATES FOR TMC}
set.seed(1)
county.focus<-county.daily%>% 
  filter(County %in% c('Austin', 'Brazoria', 'Chambers', 'Fort Bend',
                       'Galveston', 'Harris', 'Liberty', 'Montgomery', 'Waller')) %>%
  dplyr::select(-County, -PHR, -PHR_Name) %>% 
  melt(id="Date") %>% 
  dcast(Date~variable, sum, na.rm=TRUE)

# run covid.rt function on count.focus.cast dataframe
county.focus.rt.output<-covid.rt(county.focus, threshold=case_quant) %>% na.omit()

#create directory for focused plots
rt_TMC_path="rt_estimates_for_TMC"
ifelse(!dir.exists(rt_TMC_path), dir.create(rt_TMC_path), "focued area rt folder already exists")

#write csv of estimates to rt_TMC Path
write.csv(county.focus.rt.output, paste(rt_TMC_path,"/nine_county_rt_estimates.csv",sep=""), row.names = FALSE)


# plot with shaded confidence intervals
focused.rt.plot<-function(rt.data){
  library(ggplot2)
    plot<-ggplot(rt.data, aes(Date, Rt))+
    geom_ribbon(aes(ymin=lower, ymax=upper), fill="light grey")+
    geom_point(color="blue", size=0.2)+
    labs(title="R(t) Estimates:", 
    subtitle="Austin, Brazoria, Chambers, Fort Bend, Galveston, Harris, Liberty, Montgomery, and Waller")
  plot
}

county.daily%>% 
  filter(County %in% c('Austin', 'Brazoria', 'Chambers', 'Fort Bend',
                       'Galveston', 'Harris', 'Liberty', 'Montgomery', 'Waller')) %>%
  filter(Date > as.Date('2020-09-01')) 

county.focus %>% tail(14)


county.focus.plot<-focused.rt.plot(county.focus.rt.output)
ggsave( paste(rt_TMC_path,"/nine_county_rt_plot.png",sep=""), width=20, height=10, unit="cm")
```


## Metro

```{r metro rt}
#apply the covid.rt function to the metro.daily data by metro region
metro.rt.output <- nlme::gapply(metro.daily, FUN=covid.rt, groups=metro.daily$Metro_Area,
                                threshold = case_quant)

#convert list of data frames to single data frame with metro as id col
metro.rt.df <- data.table::rbindlist(metro.rt.output, idcol=TRUE)
colnames(metro.rt.df)[1] = 'Metro_Area'
```

## TSA

```{r TSA rt plot function}
# plot with shaded confidence intervals
rt.plot<-function(rt.data, caption){
  library(ggplot2)
    caption<-rt.data$TSA[1]
  plot<-ggplot(rt.data, aes(Date, Rt))+
    geom_ribbon(aes(ymin=lower, ymax=upper), fill="light grey")+
    geom_point(color="blue", size=0.2)+
    labs(title=caption)
  plot
}

```


```{r tsa rt}
#apply the covid.rt function to the TSA.daily data by TSA region
TSA.rt.output <- nlme::gapply(TSA.daily, FUN=covid.rt, groups=TSA.daily$TSA,
                              threshold = case_quant)

#convert list of data frames to single data frame with TSA as variable
TSA.rt.df <- data.table::rbindlist(TSA.rt.output, idcol=TRUE)

# add TSA_Name & fix dates
colnames(TSA.rt.df)[1] <- 'TSA'
TSA.rt.df$Date <- as.Date(TSA.rt.df$Date)
TSA.rt.df <- merge(TSA.rt.df, TSA.daily[, c('TSA', 'TSA_Name', 'Date')], by = c('TSA', 'Date'))

# combine TSA 
TSA.rt.df$TSA <- paste0(TSA.rt.df$TSA, ' - ', TSA.rt.df$TSA_Name)
TSA.rt.df$TSA_Name <- NULL

#plot Rt by TSA
nlme::gapply(TSA.rt.df, FUN=rt.plot, groups=TSA.rt.df$TSA)
```

## PHR

```{r}
phr.rt.output <- nlme::gapply(phr.daily, FUN=covid.rt, groups=phr.daily$PHR,
                              threshold = case_quant)

#convert list of data frames to single data frame with phr as id col
phr.rt.df <- data.table::rbindlist(phr.rt.output, idcol=TRUE)

# add PHR_Name & fix dates
colnames(phr.rt.df)[1] <- 'PHR'
phr.rt.df$Date <- as.Date(phr.rt.df$Date)
phr.rt.df <- merge(phr.rt.df, phr.daily[, c('PHR', 'PHR_Name', 'Date')], by = c('PHR', 'Date'))

# combine phr 
phr.rt.df$PHR <- paste0(phr.rt.df$PHR, ' - ', phr.rt.df$PHR_Name)
phr.rt.df$PHR_Name <- NULL
```


## State

```{r state rt}
state.rt.df <- covid.rt(state.daily, threshold = case_quant)
```

## export

```{r export rt output to .csv files}
write.csv(county.rt.df,"statistical-output/rt/county_rt.csv", row.names = FALSE)
write.csv(metro.rt.df, "statistical-output/rt/metro_rt.csv", row.names = FALSE)
write.csv(TSA.rt.df, "statistical-output/rt/tsa_rt.csv", row.names = FALSE)
write.csv(phr.rt.df, "statistical-output/rt/phr_rt.csv", row.names = FALSE)
write.csv(state.rt.df, "statistical-output/rt/state_rt.csv", row.names = FALSE)
```

## grouping

```{r}
colnames(county.rt.df)[1] <- 'Level'
colnames(metro.rt.df)[1] <- 'Level'
colnames(TSA.rt.df)[1] <- 'Level'
colnames(phr.rt.df)[1] <- 'Level'
state.rt.df$Level <- 'Texas'

county.rt.df$Level_Type = 'County'
metro.rt.df$Level_Type = 'Metro'
TSA.rt.df$Level_Type = 'TSA'
phr.rt.df$Level_Type = 'PHR'
state.rt.df$Level_Type = 'State'

combined.rt.df <- rbind(subset(county.rt.df, Date != max(Date)),
                        subset(metro.rt.df, Date != max(Date)),
                        subset(TSA.rt.df, Date != max(Date)),
                        subset(state.rt.df, Date != max(Date)),
                        subset(phr.rt.df, Date != max(Date)))

write.csv(combined.rt.df, 'statistical-output/rt/stacked_rt.csv', row.names = FALSE)
write.csv(combined.rt.df, 'tableau/stacked_rt.csv', row.names = FALSE)
```


# TIME SERIES

```{r ts data clean function}
#ts.rt.data.clean function will clean data, dropping unwanted variables
ts.data.clean<-function(covid.data) {

  ts_cols <- c("County", "Date", "TSA", "TSA_Name", "PHR", "PHR_Name",
               "Metro_Area", "Cases_Daily", "Cases_Cumulative",
               "Population_DSHS", "Hospitalizations_Total")
  
  covid.data <- covid.data[, names(covid.data) %in% ts_cols]
  
  # set correct types
  covid.data$Date <- as.Date(covid.data$Date)
  covid.data$Cases_Daily <- as.numeric(covid.data$Cases_Daily)
  
  #change any Cases_Daily below zero to zero
  covid.data <- covid.data %>% mutate(Cases_Daily = ifelse(Cases_Daily < 0, 0, Cases_Daily))
  return(covid.data)
}

```


```{r ts separate dataframe function}

#separate county, metro, TSA and State data into separate Data frames
ts.data.organize<-function(mycounty, mytsa, myphr){

  ### COUNTY ###
  # clean county vals and restrict to first date of collection
  cleaned.county <- ts.data.clean(mycounty)
  cleaned.county <- subset(cleaned.county, !is.na(Date) & Date >= as.Date('2020-03-04'))

  # Set column names
  county.df <- cleaned.county%>%dplyr::select(-TSA_Name, -Metro_Area)
  county.df$Level <- 'County'
  colnames(county.df)[2] = 'Level_Name'
  
  #change hospitalizations to numeric
  mytsa$Hospitalizations_Total<-as.numeric(mytsa$Hospitalizations_Total)
  
  
  # PHR 
  PHR.df <- ts.data.clean(myphr)
  PHR.df$Level = 'PHR'
  colnames(PHR.df)[3] = 'Level_Name' 
  
  ### METRO ###
  metro.temp<-cleaned.county %>%
    group_by(Metro_Area, Date) %>%
    mutate(metro_Cases_Daily = sum(Cases_Daily, na.rm = TRUE)) %>%
    mutate(metro_Cases_Cumulative = sum(Cases_Cumulative, na.rm=TRUE)) %>%
    mutate(metro_pop_DSHS = sum(Population_DSHS, na.rm=TRUE)) %>%
    dplyr::select(Date, Metro_Area, metro_Cases_Daily, metro_Cases_Cumulative, metro_pop_DSHS) %>%
    distinct()
    
  metro.df <- data.frame(Date = metro.temp$Date,
                         Level = 'metro',
                         Level_Name = metro.temp$Metro_Area,
                         Cases_Daily = metro.temp$metro_Cases_Daily,
                         Cases_Cumulative = metro.temp$metro_Cases_Cumulative,
                         Population_DSHS = metro.temp$metro_pop_DSHS)
  
  # drop NA dates
  metro.df <- subset(metro.df, !is.na(Date) & Date>=as.Date('2020-03-04'))
  
  ### TSA ###
  TSA.df <- ts.data.clean(mytsa)
  TSA.df <- subset(TSA.df, !is.na(Date) & Date >= as.Date('2020-03-04'))
  
  TSA.df$Level <- 'TSA'
  colnames(TSA.df)[2] <- 'Level_Name'
  
  
  ### STATE ###
  state.temp<-TSA.df %>%
    group_by(Date) %>%
    mutate(state_Cases_Daily = sum(Cases_Daily, na.rm = TRUE)) %>%
    mutate(state_Cases_Cumulative = sum(Cases_Cumulative, na.rm = TRUE)) %>%
    mutate(state_pop_DSHS = sum(Population_DSHS, na.rm = TRUE)) %>%
    mutate(state_hosp = sum(Hospitalizations_Total, na.rm=TRUE))%>%
    dplyr::select(Date, state_Cases_Daily, state_Cases_Cumulative, state_pop_DSHS, state_hosp) %>%
    distinct()
    
  state.df <- data.frame(Date=state.temp$Date,
                         Level = 'State',
                         Level_Name = 'Texas',
                         Cases_Daily=state.temp$state_Cases_Daily,
                         Cases_Cumulative=state.temp$state_Cases_Cumulative, 
                         Population_DSHS=state.temp$state_pop_DSHS,
                         Hospitalizations_Total=state.temp$state_hosp)
  
  state.df <- subset(state.df, Date>=as.Date('2020-03-04'))
  
  
  ### OUTPUT ###
  ts.df.out <- list(county=county.df, TSA=TSA.df, state=state.df, metro=metro.df, phr = PHR.df)
  return(ts.df.out)
}

```

```{r ts arima function}
# Compute forecast (UPDATE PREDICTION PERIOD [days] AS NEEDED)
covid.arima.forecast<-function(mydata, prediction.period = 10, mindate, threshold) {
  print(as.character(mydata[1,2]))

  maxdate <- max(mydata$Date)
  # mindate <- as.Date('2020-05-01')
  pred_start_label = format(mindate, format = '%m_%d')
  
  mydata = subset(mydata, Date >= mindate)
  model.length <- as.numeric(length(mydata$Date) + prediction.period)
  
  recent_case_avg = mydata %>% filter(Date > seq(date_out, length = 2, by = "-1 month")[2]) %>%
    summarize(mean(Cases_Daily, na.rm = TRUE)) %>% 
    unlist()
  
  print(recent_case_avg)

  if(recent_case_avg >= threshold) {
    # arima requires cases to be a timeseries vector
    # convert daily cases to time series
    my.timeseries<-ts(mydata$Cases_Daily)
    #load package(pracma)
    library(pracma)
    my.timeseries<-movavg(my.timeseries,7,"s")
    #d=0 restricts first differencing to 0 so that daily cases aren't differenced
    
    arima.fit <- forecast::auto.arima(my.timeseries)

    # omitted for performance
    # TODO: replace with p,d,q dataframe
    # # obtain diagnostic plots for ideal arima (p,d,q) selection 
    # acf <- ggAcf(my.timeseries, lag.max=30)
    # pacf <- ggPacf(my.timeseries, lag.max=30)
    # ts.diagnostics <- grid.arrange(acf, pacf, nrow=2)
    # ggsave(paste0('statistical-output/time-series/diagnostics/',
    #               mydata$Level[1],'/', mydata$Level_Name[1], pred_start_label, '.png'),
    #        plot = ts.diagnostics)

    # save parameters from arima autofit
    p<- arima.fit$arma[1]          # autoregressive order 
    q<- arima.fit$arma[2]          # moving average order 
    d<-arima.fit$arma[6]           # differencing order from model
  
    # 10 day forecast, CI for lower and upper has confidence level 95% set by level =c(95,95)
    arima.forecast <- forecast::forecast(arima.fit, h = prediction.period, level=c(95,95))

    #return a dataframe of the arima model(Daily cases by date)
    arima.out <- data.frame(Date = seq(mindate, maxdate + prediction.period, by = 'days'),
                            Cases_Raw = c(mydata$Cases_Daily, rep(NA, times = prediction.period)),
                            Cases_Daily = c(my.timeseries, arima.forecast[['mean']]),
                            CI_Lower = c(rep(NA, times = length(my.timeseries)),
                                         arima.forecast[['lower']][, 2]),
                            CI_Upper = c(rep(NA, times = length(my.timeseries)),
                                         arima.forecast[['upper']][, 2]))
                            # Order_AutoReg = c(rep(p, times = model.length)),
                            # Order_Moving_Avg = c(rep(q, times = model.length)),
                            # Differencing = c(rep(d, times = model.length)))
    
      # save prediction plot for preliminary review
      arima.plot <- ggplot(arima.out, aes(x=Date, y = Cases_Daily))+
                    geom_ribbon(aes(ymin = CI_Lower, ymax = CI_Upper), fill = "red", alpha = 0.5, size = 0.1)+
                    geom_line(color = "black", size = 1)+
                    labs(y = 'Daily Cases (7-Day Moving Average)', x = 'Date',
                         title = mydata$Level_Name[1]) +
                    scale_x_date(limits = c(mindate, maxdate + prediction.period),
                                 date_labels = '%b-%d', date_breaks = '1 week') + 
                    ggpubr::theme_pubr() +
                    theme(axis.text.x = element_text(angle = -90))
      
      ggsave(plot = arima.plot, paste0('statistical-output/time-series/plots/',
                                       mydata$Level[1],'/', mydata$Level_Name[1], pred_start_label, '.png'))    
    
    } else {
    # insufficient data catch: return NA values for predictions 
    arima.out <- data.frame(Date = seq(mindate, maxdate + prediction.period, by = 'days'),
                            Cases_Raw = c(mydata$Cases_Daily, rep(NA, times = prediction.period)),
                            Cases_Daily = rep(NA, times = model.length),
                            CI_Lower = rep(NA, times = model.length),
                            CI_Upper =  rep(NA, times = model.length))
                            # Order_AutoReg = c(rep(NA, times = model.length)),
                            # Order_Moving_Avg = c(rep(NA, times = model.length)),
                            # Differencing = c(rep(NA, times= model.length)))
    }
  
  #replace CI lower limit with 0 if negative
  arima.out$CI_Lower <- ifelse(arima.out$CI_Lower>=0 ,arima.out$CI_Lower, 0)
  
  return(arima.out)
}

```

```{r ts Get cleaned dataframes, include=FALSE}
#run ts.data.organize function on full.dshs and tsa.dshs data
covid.frame.list <- ts.data.organize(full.dshs, tsa.dshs, phr.df)

#extract data frames from the list
county.Daily <- covid.frame.list$county
TSA.Daily <- covid.frame.list$TSA
state.Daily <- covid.frame.list$state
metro.Daily <- covid.frame.list$metro
PHR.Daily <- covid.frame.list$phr
```

## County

```{r county forecasts, output = NULL}
# apply arima across all counties
county.arima.output.1 <- nlme::gapply(county.Daily,
                                    FUN = covid.arima.forecast,
                                    groups = county.Daily$Level_Name,
                                    mindate = as.Date('2020-03-04'),
                                    threshold = case_quant)

# bind list of dataframes to one dataframe
county.arima.df.1 <- data.table::rbindlist(county.arima.output.1, idcol = TRUE)
colnames(county.arima.df.1)[1] <- 'County'

# county.arima.output.2 <- nlme::gapply(county.Daily,
#                                     FUN = covid.arima.forecast,
#                                     groups = county.Daily$Level_Name,
#                                     mindate = as.Date('2020-06-03'))
# 
# 
# # bind list of dataframes to one dataframe
# county.arima.df.2 <- data.table::rbindlist(county.arima.output.2, idcol = TRUE)
# colnames(county.arima.df.2)[1] <- 'County'
```

## Metro

```{r metro forecasts}
# apply arima across both metro values
metro.arima.output.1 <- nlme::gapply(metro.Daily,
                                   FUN = covid.arima.forecast,
                                   groups = metro.Daily$Level_Name,
                                   mindate = as.Date('2020-03-04'),
                                   threshold = case_quant)

# bind list of dataframes to one dataframe
metro.arima.df.1 <- data.table::rbindlist(metro.arima.output.1, idcol = TRUE)
colnames(metro.arima.df.1)[1] <- 'Metro_Area'
# 
# metro.arima.output.2 <- nlme::gapply(metro.Daily,
#                                    FUN = covid.arima.forecast,
#                                    groups = metro.Daily$Level_Name,
#                                    mindate = as.Date('2020-06-03'))
# 
# # bind list of dataframes to one dataframe
# metro.arima.df.2 <- data.table::rbindlist(metro.arima.output.2, idcol = TRUE)
# colnames(metro.arima.df.2)[1] <- 'Metro_Area'
```


## TSA

```{r TSA forecasts}
# All data
TSA.arima.output.1 <- nlme::gapply(TSA.Daily,
                                   FUN = covid.arima.forecast,
                                   groups = TSA.Daily$Level_Name,
                                   mindate = as.Date('2020-03-04'),
                                   threshold = case_quant)

TSA.arima.df.1 <- data.table::rbindlist(TSA.arima.output.1, idcol=TRUE)

colnames(TSA.arima.df.1)[1] <- 'TSA'
TSA.arima.df.1 <- merge(TSA.arima.df.1, unique(TSA.daily[, c('TSA', 'TSA_Name')]), by = c('TSA'))

# combine TSA 
TSA.arima.df.1$TSA <- paste0(TSA.arima.df.1$TSA, ' - ', TSA.arima.df.1$TSA_Name)
TSA.arima.df.1$TSA_Name <- NULL
```


## PHR

```{r}
PHR.arima.output <- nlme::gapply(PHR.Daily,
                                 FUN = covid.arima.forecast,
                                 groups = PHR.Daily$Level_Name,
                                 mindate = as.Date('2020-03-04'),
                                 threshold = case_quant)

PHR.arima.df <- data.table::rbindlist(PHR.arima.output, idcol=TRUE)
colnames(PHR.arima.df)[1] <- 'Level_Name'

PHR.arima.df <- merge(PHR.arima.df, unique(PHR.Daily[, c('PHR', 'Level_Name')]), by = c('Level_Name'))

# combine PHR 
PHR.arima.df$Level_Name <- paste0(PHR.arima.df$PHR, ' - ', PHR.arima.df$Level_Name)
PHR.arima.df$PHR <- NULL
colnames(PHR.arima.df)[1] <- 'PHR'
```

## State

```{r Texas forecasts, results='hide'}
texas.arima.df.1 <- covid.arima.forecast(state.Daily,
                                         mindate = as.Date('2020-03-04'),
                                         threshold = case_quant)
# texas.arima.df.2 <- covid.arima.forecast(state.Daily, mindate = as.Date('2020-06-03'))
```

<!-- ## TimeSeriesPlots -->


```{r TSA and Texas plots}
#Create plot function, forecast.data is the dataframe containing true data
# and the forecast data. days.forecast is the number of days that are forecast
# in the dataframe (here we are forecasting 10 days, so it should be 10 unless we
# change this at some point down the road)
forecast.plot<-function(forecast.data, plot.title, y.label){
    mindate<-as.POSIXct("2020-03-04")
    maxdate<-as.POSIXct(max(forecast.data$Date))
  ts.plot<-ggplot(aes(x=as.POSIXct(Date), y=Cases_Daily), data=forecast.data)+
    geom_ribbon(aes(ymin=CI_Lower, ymax=CI_Upper), fill="grey50", size=0.1)+
    geom_line(color="blue", size=1)+
    geom_line(aes(x=as.POSIXct(Date), y=CI_Lower), color="grey", size=0.1)+
    geom_line(aes(x=as.POSIXct(Date),y=CI_Upper), color="grey", size=0.1)+
    scale_x_datetime(limits = c(mindate, maxdate))+
    xlab("Date")+
    ylab(y.label)+
    #Can use the following title if we are running using nlme for data frame
    #ggtitle(paste(forecast.data$.id,": TS Daily Cases + 10 Day Forcast",sep=""))
    ggtitle(plot.title)
ts.plot
}
```

```{r Arima select forecast plots }

######### View Output Table & Graph for TSA Q ##########
library(kableExtra)
#subset TSA Q arima data
TSA_Q.arima.df<-TSA.arima.df.1%>%subset(TSA.arima.df.1$TSA== "Q - Houston")
#apply the plot function to TSA Q arima data frame
TSA_Q.arima.df%>%kable(caption="Greater Houston ARIMA - Daily Cases")%>%kable_styling(full_width=FALSE)
#nlme::gapply(TSA.arima.df, FUN = forecast.plot, groups=TSA.daily$Level_Name)
forecast.plot(TSA_Q.arima.df, "TSA Q - Greater Houston Daily Cases", "Daily Cases")

######### View Output Table & Graph for Texas ##########
texas.arima.df.1%>%kable(caption="Texas Arima - Daily Cases")%>%kable_styling(full_width=FALSE)
forecast.plot(texas.arima.df.1, "Texas Daily Cases", "Daily Cases")
```

## export

```{r export arima results}
write.csv(texas.arima.df.1, 'statistical-output/time-series/state_case_timeseries.csv', row.names = FALSE)
write.csv(TSA.arima.df.1, 'statistical-output/time-series/tsa_case_timeseries.csv', row.names = FALSE)
write.csv(county.arima.df.1, 'statistical-output/time-series/county_case_timeseries.csv', row.names = FALSE)
write.csv(metro.arima.df.1, 'statistical-output/time-series/metro_case_timeseries.csv', row.names = FALSE)
write.csv(PHR.arima.df, 'statistical-output/time-series/phr_case_timeseries.csv', row.names = FALSE)

# write.csv(texas.arima.df.2, 'statistical-output/time-series/state_timeseries_06_03.csv', row.names = FALSE)
# write.csv(TSA.arima.df.2, 'statistical-output/time-series/tsa_timeseries_06_03.csv', row.names = FALSE)
# write.csv(county.arima.df.2, 'statistical-output/time-series/county_timeseries_06_03.csv', row.names = FALSE)
# write.csv(metro.arima.df.2, 'statistical-output/time-series/metro_timeseries_06_03.csv', row.names = FALSE)
```

## grouping

```{r}
colnames(county.arima.df.1)[1] <- 'Level'
colnames(metro.arima.df.1)[1] <- 'Level'
colnames(TSA.arima.df.1)[1] <- 'Level'
texas.arima.df.1$Level <- 'Texas'
colnames(PHR.arima.df)[1] <- 'Level'

county.arima.df.1$Level_Type = 'County'
metro.arima.df.1$Level_Type = 'Metro'
TSA.arima.df.1$Level_Type = 'TSA'
PHR.arima.df$Level_Type = 'PHR'
texas.arima.df.1$Level_Type = 'State'


combined.arima.df.1 <- rbind(county.arima.df.1, metro.arima.df.1, TSA.arima.df.1, texas.arima.df.1, PHR.arima.df)
write.csv(combined.arima.df.1, 'statistical-output/time-series/stacked_case_timeseries.csv',
          row.names = FALSE)

write.csv(combined.arima.df.1, 'tableau/stacked_case_timeseries.csv',
          row.names = FALSE)
# 
# combined.arima.df.2 <- rbind(county.arima.df.2, metro.arima.df.2, TSA.arima.df.2, texas.arima.df.2)
# write.csv(combined.arima.df.2, 'statistical-output/time-series/stacked_timeseries_06_03.csv',
#           row.names = FALSE)
# 
# write.csv(combined.arima.df.2, 'tableau/stacked_timeseries_06_03.csv',
#           row.names = FALSE)

```

# Hospitalization Time Series
```{r hospitalization arima function}
# Compute forecast (UPDATE PREDICTION PERIOD [days] AS NEEDED)
covid.arima.forecast<-function(mydata, prediction.period = 10, mindate)
{
  maxdate <- max(mydata$Date)
  # mindate <- as.Date('2020-05-01')
  pred_start_label = format(mindate, format = '%m_%d')
  
  mydata = subset(mydata, Date >= mindate)
  model.length <- as.numeric(length(mydata$Date) + prediction.period)

  if(max(mydata$Hospitalizations_Total>=100, na.rm = TRUE))
  {

    
    # arima requires cases to be a timeseries vector
    #convert daily cases to time series
    my.timeseries<-ts(mydata$Hospitalizations_Total)
    #my.timeseries <- rollmeanr(my.timeseries, k=7, na.pad=TRUE, align = 'right')
    
    #load package(pracma)
    library(pracma)
    my.timeseries<-movavg(my.timeseries,7,"s")
    #d=0 restricts first differencing to 0 so that daily cases aren't differenced
    
    arima.fit <- forecast::auto.arima(my.timeseries)
  
    # obtain diagnostic plots for ideal arima (p,d,q) selection 
    acf <- ggAcf(my.timeseries, lag.max=30)
    pacf <- ggPacf(my.timeseries, lag.max=30)
    ts.diagnostics <- grid.arrange(acf, pacf, nrow=2)
    ggsave(paste0('statistical-output/time-series/diagnostics/',
                  mydata$Level[1],'/', mydata$Level_Name[1], pred_start_label, '.png'),
           plot = ts.diagnostics)

    
    
    # save parameters from arima autofit
    p<- arima.fit$arma[1]          # autoregressive order 
    q<- arima.fit$arma[2]          # moving average order 
    d<-arima.fit$arma[6]           # differencing order from model
  
    # 10 day forecast, CI for lower and upper has confidence level 95% set by level =c(95,95)
    arima.forecast <- forecast::forecast(arima.fit, h = prediction.period, level=c(95,95))

    #return a dataframe of the arima model(Daily cases by date)
    arima.out <- data.frame(Date = seq(mindate, maxdate + prediction.period, by = 'days'),
                            # Cases_Raw = c(mydata$Hospitalizations_Total, rep(NA, times = prediction.period)),
                            Hospitalizations_Total = c(my.timeseries, arima.forecast[['mean']]),
                            CI_Lower = c(rep(NA, times = length(my.timeseries)),
                                         arima.forecast[['lower']][, 2]),
                            CI_Upper = c(rep(NA, times = length(my.timeseries)),
                                         arima.forecast[['upper']][, 2]))
                            # Order_AutoReg = c(rep(p, times = model.length)),
                            # Order_Moving_Avg = c(rep(q, times = model.length)),
                            # Differencing = c(rep(d, times = model.length)))
    
      # save prediction plot for preliminary review
      arima.plot <- ggplot(arima.out, aes(x=Date, y = Hospitalizations_Total))+
                    geom_ribbon(aes(ymin = CI_Lower, ymax = CI_Upper), fill = "red", alpha = 0.5, size = 0.1)+
                    geom_line(color = "black", size = 1)+
                    labs(y = 'Daily Cases (7-Day Moving Average)', x = 'Date',
                         title = mydata$Level_Name[1]) +
                    scale_x_date(limits = c(mindate, maxdate + prediction.period),
                                 date_labels = '%b-%d', date_breaks = '1 week') + 
                    ggpubr::theme_pubr() +
                    theme(axis.text.x = element_text(angle = -90))
      
      ggsave(plot = arima.plot, paste0('statistical-output/time-series/plots/',
                                       mydata$Level[1],'/', mydata$Level_Name[1], pred_start_label, '.png'))    
    
    } else {
    # insufficient data catch: return NA values for predictions 
    arima.out <- data.frame(Date = seq(mindate, maxdate + prediction.period, by = 'days'),
                            # Cases_Raw = c(mydata$Hospitalizations_Total, rep(NA, times = prediction.period)),
                            Hospitalizations_Total = rep(NA, times = model.length),
                            CI_Lower = rep(NA, times = model.length),
                            CI_Upper =  rep(NA, times = model.length))
                            # Order_AutoReg = c(rep(NA, times = model.length)),
                            # Order_Moving_Avg = c(rep(NA, times = model.length)),
                            # Differencing = c(rep(NA, times= model.length)))
    }
  
  #replace CI lower limit with 0 if negative
  arima.out$CI_Lower <- ifelse(arima.out$CI_Lower>=0,arima.out$CI_Lower, 0)
  
  
  return(arima.out)
}

```

## TSA - Hospitalization

```{r TSA Hosp forecasts}
# All data
TSA.hosp.arima.output.1 <- nlme::gapply(TSA.Daily,
                                   FUN = covid.arima.forecast,
                                   groups = TSA.Daily$Level_Name,
                                   mindate = as.Date('2020-03-04'))

TSA.hosp.arima.df.1 <- data.table::rbindlist(TSA.hosp.arima.output.1, idcol=TRUE)

colnames(TSA.hosp.arima.df.1)[1] <- 'TSA'
TSA.hosp.arima.df.1 <- merge(TSA.hosp.arima.df.1, unique(TSA.daily[, c('TSA', 'TSA_Name')]), by = c('TSA'))

# combine TSA 
TSA.hosp.arima.df.1$TSA <- paste0(TSA.hosp.arima.df.1$TSA, ' - ', TSA.hosp.arima.df.1$TSA_Name)
TSA.hosp.arima.df.1$TSA_Name <- NULL
```

## State - hospitalization

```{r Texas Hosp forecasts, results='hide'}
texas.hosp.arima.df.1 <- covid.arima.forecast(state.Daily, mindate = as.Date('2020-03-04'))
# texas.hosp.arima.df.2 <- covid.arima.forecast(state.Daily, mindate = as.Date('2020-06-03'))
```

## Hospitalization Write to CSV
```{r Hospitalizations to csv}
tsa.hosp.arima<-write.csv(TSA.hosp.arima.df.1, "statistical-output/time-series/tsa_hosp_timeseries.csv", row.names=FALSE)
texas.hosp.arima<-write.csv(texas.hosp.arima.df.1, "statistical-output/time-series/state_hosp_timeseries.csv", row.names=FALSE)
```

## Stacking
```{r}
TSA_hosp_out = TSA.hosp.arima.df.1
colnames(TSA_hosp_out)[1] = 'Level'
texas.hosp.arima.df.1$Level = 'Texas'


TSA_hosp_out$Level_Type = 'TSA'
texas.hosp.arima.df.1$Level_Type = 'State'

combined.hosp.arima.df.1 <- rbind(TSA_hosp_out, texas.hosp.arima.df.1)
write.csv(combined.hosp.arima.df.1, 'statistical-output/time-series/stacked_hosp_timeseries.csv',
          row.names = FALSE)

write.csv(combined.hosp.arima.df.1, 'tableau/stacked_hosp_timeseries.csv',
          row.names = FALSE)
```


## Hospitalization Forecast Plots

```{r TSA and Texas plots Hosp}
#Create plot function, forecast.data is the dataframe containing true data
# and the forecast data. days.forecast is the number of days that are forecast
# in the dataframe (here we are forecasting 10 days, so it should be 10 unless we
# change this at some point down the road)
forecast.plot<-function(forecast.data, plot.title, y.label){
    mindate<-as.POSIXct("2020-03-04")
    maxdate<-as.POSIXct(max(forecast.data$Date))
  ts.plot<-ggplot(aes(x=as.POSIXct(Date), y=Hospitalizations_Total), data=forecast.data)+
    geom_ribbon(aes(ymin=CI_Lower, ymax=CI_Upper), fill="grey50", size=0.1)+
    geom_line(color="blue", size=1)+
    geom_line(aes(x=as.POSIXct(Date), y=CI_Lower), color="grey", size=0.1)+
    geom_line(aes(x=as.POSIXct(Date),y=CI_Upper), color="grey", size=0.1)+
    scale_x_datetime(limits = c(mindate, maxdate))+
    xlab("Date")+
    ylab(y.label)+
    #Can use the following title if we are running using nlme for data frame
    #ggtitle(paste(forecast.data$.id,": TS Daily Cases + 10 Day Forcast",sep=""))
    ggtitle(plot.title)
ts.plot
}
```

```{r Arima Hosp Forecast Plots }

######### View Output Table & Graph for TSA Q ##########
library(kableExtra)
#subset TSA Q arima data
TSA_Q.arima.df<-TSA.hosp.arima.df.1%>%subset(TSA.hosp.arima.df.1$TSA == "Q - Houston")
#apply the plot function to TSA Q arima data frame
TSA_Q.arima.df%>%kable(caption="Greater Houston ARIMA - Daily Hosp")%>%kable_styling(full_width=FALSE)
#nlme::gapply(TSA.hosp.arima.df, FUN = forecast.plot, groups=TSA.daily$Level_Name)
forecast.plot(TSA_Q.arima.df, "TSA Q - Greater Houston Daily Hosp", "Daily Hosp")

######### View Output Table & Graph for Texas ##########
texas.hosp.arima.df.1%>%kable(caption="Texas Arima - Daily Hosp")%>%kable_styling(full_width=FALSE)
forecast.plot(texas.hosp.arima.df.1, "Texas Daily Hospitalizations", "Daily Hosp")
```

# STANDARD STATISTICAL TESTS

## CASE RATIOS

### TSA

```{r}
#TODO: update all varnames
cumcases.TSA <- TSA.Daily %>% dplyr::select(Date,Level_Name, Cases_Cumulative) %>% filter(!is.na(Level_Name))
cumcases.TSA$Date <- ymd(cumcases.TSA$Date)

latestdate <- max(cumcases.TSA$Date)
earliestdate <- latestdate - 14
middate <- latestdate - 7

week2 <- subset(cumcases.TSA, Date==latestdate, select=Cases_Cumulative) - 
  subset(cumcases.TSA, Date==middate, select=Cases_Cumulative)

week1 <- subset(cumcases.TSA, Date==middate, select=Cases_Cumulative) - 
  subset(cumcases.TSA, Date==earliestdate, select=Cases_Cumulative)

current_ratio <- (week2/week1)[,1]
#current_ratio[week1<10] <- NA
current_ratio[current_ratio<0] <- NA
tsa_current_ratio_dat <- data.frame(TSA=subset(cumcases.TSA, Date==latestdate, select=Level_Name)[,1],
                                    current_ratio=current_ratio, current_ratio_cat = cut(current_ratio, breaks=c(0,0.5,.9,1.1,1.5,2,4,8,max(current_ratio))))

# add TSA name
tsa_current_ratio_dat <- merge(tsa_current_ratio_dat, unique(TSA.daily[, c('TSA', 'TSA_Name')]), by = 'TSA')

# combine TSA name
tsa_current_ratio_dat$TSA <- paste0(tsa_current_ratio_dat$TSA, ' - ', tsa_current_ratio_dat$TSA_Name)
tsa_current_ratio_dat$TSA_Name <- NULL
```

### PHR

```{r}
cumcases.PHR <- PHR.Daily %>% dplyr::select(Date, Level_Name, Cases_Cumulative) %>% filter(!is.na(Level_Name))
cumcases.PHR$Date <- ymd(cumcases.PHR$Date)

latestdate <- max(cumcases.PHR$Date)
earliestdate <- latestdate - 14
middate <- latestdate - 7

week2 <- subset(cumcases.PHR, Date==latestdate, select=Cases_Cumulative) - 
  subset(cumcases.PHR, Date==middate, select=Cases_Cumulative)

week1 <- subset(cumcases.PHR, Date==middate, select=Cases_Cumulative) - 
  subset(cumcases.PHR, Date==earliestdate, select=Cases_Cumulative)

current_ratio <- (week2/week1)[,1]
#current_ratio[week1<10] <- NA
current_ratio[current_ratio<0] <- NA
phr_current_ratio_dat <- data.frame(Level_Name=subset(cumcases.PHR, Date==latestdate, select=Level_Name)[,1],
                                    current_ratio=current_ratio, current_ratio_cat = cut(current_ratio, breaks=c(0,0.5,.9,1.1,1.5,2,4,8,max(current_ratio))))

# add PHR name
phr_current_ratio_dat <- merge(phr_current_ratio_dat, unique(PHR.Daily[, c('PHR', 'Level_Name')]), by = 'Level_Name')

# combine PHR name
phr_current_ratio_dat$Level_Name <- paste0(phr_current_ratio_dat$PHR, ' - ', phr_current_ratio_dat$Level_Name)
phr_current_ratio_dat$PHR <- NULL

colnames(phr_current_ratio_dat)[1] <- 'PHR'
```


### County

```{r}
# declare vals
cumcases.county <- county.Daily %>% dplyr::select(Date, Level_Name, Cases_Cumulative) %>% filter(!is.na(Level_Name))
cumcases.county$Date <- ymd(cumcases.county$Date)

latestdate <- max(cumcases.county$Date, na.rm=T)
earliestdate <- latestdate - 14
middate <- latestdate-7

# calc cumulative case ratios from 2 weeks ago and last week
week2 <- subset(cumcases.county, Date==latestdate, select=Cases_Cumulative) - 
         subset(cumcases.county, Date==middate, select=Cases_Cumulative)

week1 <- subset(cumcases.county, Date==middate, select=Cases_Cumulative) - 
         subset(cumcases.county, Date==earliestdate, select=Cases_Cumulative)

current_ratio <- (week2/week1)[,1]

# add contingencies 
current_ratio[week1<10] <- NA
current_ratio[current_ratio<0] <- NA

# output
# TODO: investigate select vs subset(...)[, 'County']
county_current_ratio_dat <- 
  data.frame(County=subset(cumcases.county, Date==latestdate, select=Level_Name)[,1],
             current_ratio=current_ratio,
             current_ratio_cat = cut(current_ratio,
                                     breaks=c(0,0.5,.9,1.1,1.5,2,4,8,max(current_ratio))))
```

### Metro

```{r}
cumcases.metro <- metro.Daily %>% dplyr::select(Date, Level_Name, Cases_Cumulative) %>% filter(!is.na(Level_Name))
cumcases.metro$Date <- ymd(cumcases.metro$Date)

latestdate <- max(cumcases.metro$Date, na.rm=T)
earliestdate <- latestdate - 14
middate <- latestdate-7

week2 <- subset(cumcases.metro, Date==latestdate, select=Cases_Cumulative) - 
         subset(cumcases.metro, Date==middate, select=Cases_Cumulative)
week1 <- subset(cumcases.metro, Date==middate, select=Cases_Cumulative) - 
         subset(cumcases.metro, Date==earliestdate, select=Cases_Cumulative)

current_ratio <- (week2/week1)[,1]
current_ratio[current_ratio<0] <- NA

metro_current_ratio_dat <-
  data.frame(Metro_Area=subset(cumcases.metro, Date==latestdate, select=Level_Name)[,1],
             current_ratio=current_ratio,
             current_ratio_cat = cut(current_ratio,
                                     breaks=c(0,0.5,.9,1.1,1.5,2,4,8,max(current_ratio))))
```

### State

```{r}
cumcases.state <- state.Daily %>% dplyr::select(Date, Cases_Cumulative)
cumcases.state$Date <- ymd(cumcases.state$Date)

latestdate <- max(cumcases.state$Date, na.rm = TRUE)
earliestdate <- latestdate - 14
middate <- latestdate-7

week2 <- subset(cumcases.state, Date == latestdate, select = Cases_Cumulative) - 
         subset(cumcases.state, Date == middate, select = Cases_Cumulative)

week1 <- subset(cumcases.state, Date == middate, select = Cases_Cumulative) - 
         subset(cumcases.state, Date == earliestdate, select = Cases_Cumulative)

current_ratio <- (week2/week1)[,1]
current_ratio[current_ratio<0] <- NA

state_current_ratio_dat <- 
  data.frame(current_ratio=current_ratio,
             current_ratio_cat = cut(current_ratio,
                                     breaks=c(0,0.5,.9,1.1,1.5,2,4,8,max(current_ratio))))
```

### export 

```{r export ratio results}
write.csv(state_current_ratio_dat, 'statistical-output/standard-stats/case-ratios/state_case_ratio.csv', row.names = F)
write.csv(tsa_current_ratio_dat,'statistical-output/standard-stats/case-ratios/tsa_case_ratio.csv', row.names = F)
write.csv(county_current_ratio_dat, 'statistical-output/standard-stats/case-ratios/county_case_ratio.csv', row.names = F)
write.csv(metro_current_ratio_dat,'statistical-output/standard-stats/case-ratios/metro_case_ratio.csv', row.names = F)
write.csv(phr_current_ratio_dat,'statistical-output/standard-stats/case-ratios/phr_case_ratio.csv', row.names = F)

# write.csv(state_current_ratio_dat, 'tableau/case-ratios/state_case_ratio.csv', row.names = F)
# write.csv(tsa_current_ratio_dat,'tableau/case-ratios/tsa_case_ratio.csv', row.names = F)
# write.csv(county_current_ratio_dat, 'tableau/case-ratios/county_case_ratio.csv', row.names = F)
# write.csv(metro_current_ratio_dat,'tableau/case-ratios/metro_case_ratio.csv', row.names = F)
# write.csv(phr_current_ratio_dat,'tableau/case-ratios/phr_case_ratio.csv', row.names = F)
```


### grouping

```{r}
colnames(county_current_ratio_dat)[1] <- 'Level'
colnames(metro_current_ratio_dat)[1] <- 'Level'
colnames(tsa_current_ratio_dat)[1] <- 'Level'
colnames(phr_current_ratio_dat)[1] <- 'Level'
state_current_ratio_dat$Level <- 'Texas'


county_current_ratio_dat$Level_Type = 'County'
metro_current_ratio_dat$Level_Type = 'Metro'
tsa_current_ratio_dat$Level_Type = 'TSA'
phr_current_ratio_dat$Level_Type = 'PHR'
state_current_ratio_dat$Level_Type = 'State'

combined_current_ratio_dat <- rbind(county_current_ratio_dat, metro_current_ratio_dat,
                                   tsa_current_ratio_dat, state_current_ratio_dat, phr_current_ratio_dat)

stacked_ratio_out = combined_current_ratio_dat %>% 
  mutate(County = ifelse(Level_Type == 'County', as.character(Level), '')) %>% 
  mutate(TSA = ifelse(Level_Type == 'TSA', as.character(Level), '')) %>% 
  mutate(PHR = ifelse(Level_Type == 'PHR', as.character(Level), '')) %>% 
  mutate(`Metro Area` = ifelse(Level_Type == 'Metro', as.character(Level), '')) %>% 
  mutate(State = ifelse(Level_Type == 'State', as.character(Level), ''))


write.csv(stacked_ratio_out, 'statistical-output/standard-stats/case-ratios/stacked_case_ratio.csv',
          row.names = FALSE)

write.csv(stacked_ratio_out, 'tableau/stacked_case_ratio.csv',
          row.names = FALSE)
# write.csv(combined_current_ratio_dat, 'tableau/stacked_case_ratio.csv',
#           row.names = FALSE)
```


## % CHANGE

```{r}
create.case.test <-   function(level, dat, region){
  # creates the % difference in cases and tests and smooth line with CIs 
  # level: either "TSA", "county", or "metro". Note that "county" won't work for many counties unless have enough cases.
  # dat: dataset (e.g. "county", "metro", "tsa")
  # region: the region within the dataset (county, metro region, or tsa)
  
  if(level == "TSA"){
    dat <- subset(dat, TSA==region)
  }
   if(level == "PHR"){
    dat <- subset(dat, PHR==region)
  }
  if(level == "county"){
    dat <- subset(dat, County == region)
  }
  if(level == "metro"){
    dat <- subset(dat, Metro_Area==region)
  }

  # restrict data to first test date (for % test increase)
  first.test.date <- ymd("2020-04-22")
  dat$Date <- ymd(dat$Date)
  dat$Date2 <- as.numeric(ymd(dat$Date))
  
  # get 14 day rolling avg for instances where initial cases or tests are 0
  dat$cases_avg14 <- rollmean(dat$Cases_Daily, k=14, fill = NA, align = 'right', na.rm = TRUE)
  dat$tests_avg14 <- rollmean(dat$Tests_Daily, k=14, fill = NA, align = 'right', na.rm = TRUE)
  
  # restrict new df to first.test.date forward  
  slopedata.tests <- data.frame(subset(dat, select=c(Date, Date2, Cases_Daily,
                                                     Tests_Daily, cases_avg14, tests_avg14))) %>%
    subset(ymd(Date) >= ymd(first.test.date))
  
  tests_start <- as.numeric(subset(slopedata.tests, Date==first.test.date, select=Tests_Daily))
  cases_start <- as.numeric(subset(slopedata.tests, Date==first.test.date, select=Cases_Daily))
  
  # numeric date for gam                         
  tmpdata <- data.frame(Date2=slopedata.tests$Date2)

  if (cases_start != 0 & tests_start != 0) {
    
    # Calc splines if cases and tests will produce non Inf values
    slopedata.tests$newtestsY <- 100*(as.vector(slopedata.tests$Tests_Daily / tests_start) - 1)
    slopedata.tests$newcasesY <- 100*(as.vector(slopedata.tests$Cases_Daily / cases_start) - 1)
    
    # fit and predict w/ gam for spline vals
    cases.gam <- gam(newcasesY~s(Date2,bs="cs", k=5), data=slopedata.tests)
    tests.gam <- gam(newtestsY~s(Date2,bs="cs",k=5), data=slopedata.tests)
    
    cases.fit <- predict(cases.gam, tmpdata, se.fit=TRUE)
    tests.fit <- predict(tests.gam, tmpdata, se.fit=TRUE)
    
    # Use 95% CI for splines
    tmp.df <- data.frame(Date = slopedata.tests$Date,
                         Data_Type = 'Raw',
                         cases_percentdiff = slopedata.tests$newcasesY,
                         tests_percentdiff = slopedata.tests$newtestsY,
                         cases_percentdiff_spline = cases.fit$fit, 
                         cases_percentdiff_spline_lower = cases.fit$fit-1.96*cases.fit$se, 
                         cases_percentdiff_spline_upper = cases.fit$fit+1.96*cases.fit$se,
                         tests_percentdiff_spline = tests.fit$fit,
                         tests_percentdiff_spline_lower = tests.fit$fit-1.96*tests.fit$se,
                         tests_percentdiff_spline_upper = tests.fit$fit+1.96*tests.fit$se)    
  } else {
    
    # only calc % increase for regions w/ sparse cases
    cases_avg_start <- as.numeric(subset(slopedata.tests, Date==first.test.date, select=cases_avg14))
    tests_avg_start <- as.numeric(subset(slopedata.tests, Date==first.test.date, select=tests_avg14))

    slopedata.tests$newcasesY <- 100*(as.vector(slopedata.tests$cases_avg14 / cases_avg_start) - 1)
    slopedata.tests$newtestsY <- 100*(as.vector(slopedata.tests$tests_avg14 / tests_avg_start) - 1)
    
    tmp.df <- data.frame(Date = slopedata.tests$Date,
                         Data_Type = '14-Day Average',
                         cases_percentdiff = slopedata.tests$newcasesY,
                         tests_percentdiff = slopedata.tests$newtestsY,
                         cases_percentdiff_spline = rep(NA, times = nrow(slopedata.tests)),
                         cases_percentdiff_spline_lower = rep(NA, times = nrow(slopedata.tests)),
                         cases_percentdiff_spline_upper = rep(NA, times = nrow(slopedata.tests)),
                         tests_percentdiff_spline = rep(NA, times = nrow(slopedata.tests)),
                         tests_percentdiff_spline_lower = rep(NA, times = nrow(slopedata.tests)),
                         tests_percentdiff_spline_upper = rep(NA, times = nrow(slopedata.tests)))  
    }
  
  return(tmp.df)  
}
```


### State

```{r}
state <- readxl::read_xlsx('combined-datasets/state.xlsx', sheet=1)
state.case.test.df <- create.case.test(level="State", state, NA)
write.csv(state.case.test.df, 'statistical-output/standard-stats/pct-change/state_pct_change.csv',
          row.names = FALSE)
```

### TSA

```{r}
tsa <- read.csv('combined-datasets/tsa.csv')

# Create tsa-level data (can optimize this code in the future, but it runs pretty quickly already)
# TODO: convert to gapply -> rbindlist
tsalist <- unique(tsa$TSA)
tmp <- create.case.test(level="TSA", tsa,tsalist[1])
tsa.case.test.df <- data.frame(TSA=rep(tsalist[1], nrow(tmp)), 
                               TSA_Name=rep(unique(tsa$TSA_Name[1])),
                               tmp)

for(i in c(2:length(tsalist))){
  tmp <- create.case.test(level="TSA", tsa,tsalist[i])
  tsa.case.test.df <- rbind(tsa.case.test.df, data.frame(TSA=rep(tsalist[i], nrow(tmp)), 
                                                         TSA_Name=rep(unique(tsa$TSA_Name[i])),
                                                         tmp))
}


## combine tsa name
tsa.case.test.df$TSA = paste0(tsa.case.test.df$TSA, ' - ', tsa.case.test.df$TSA_Name)
tsa.case.test.df$TSA_Name = NULL

write.csv(tsa.case.test.df, 'statistical-output/standard-stats/pct-change/tsa_pct_change.csv',
          row.names = FALSE)
```


### PHR

```{r}
phr <- read.csv('combined-datasets/phr.csv')

# Create phr-level data (can optimize this code in the future, but it runs pretty quickly already)
# TODO: convert to gapply -> rbindlist
phrlist <- unique(phr$PHR)
tmp <- create.case.test(level="PHR", phr,phrlist[1])
phr.case.test.df <- data.frame(PHR=rep(phrlist[1], nrow(tmp)), 
                               PHR_Name=rep(unique(phr$PHR_Name[1])),
                               tmp)

for(i in c(2:length(phrlist))){
  tmp <- create.case.test(level="PHR", phr,phrlist[i])
  phr.case.test.df <- rbind(phr.case.test.df, data.frame(PHR=rep(phrlist[i], nrow(tmp)), 
                                                         PHR_Name=rep(unique(phr$PHR_Name[i])),
                                                         tmp))
}


## combine phr name
phr.case.test.df$PHR = paste0(phr.case.test.df$PHR, ' - ', phr.case.test.df$PHR_Name)
phr.case.test.df$PHR_Name = NULL

write.csv(phr.case.test.df, 'statistical-output/standard-stats/pct-change/phr_pct_change.csv',
          row.names = FALSE)
```


### Metro

```{r}
Metro <- read.csv('combined-datasets/metro.csv')

metrolist <- unique(Metro$Metro_Area)
 
tmp <- create.case.test(level="metro", Metro,metrolist[1])
metro.case.test.df <- data.frame(Metro_Area=rep(metrolist[1], nrow(tmp)), tmp)

tmp <- create.case.test(level="metro", Metro,metrolist[2])
metro.case.test.df <- rbind(metro.case.test.df, data.frame(Metro_Area=rep(metrolist[2], nrow(tmp)), tmp))

write.csv(metro.case.test.df, 'statistical-output/standard-stats/pct-change/metro_pct_change.csv',
          row.names = FALSE)
```


### County

```{r}
county <- read.csv('combined-datasets/county.csv')
countylist <- unique(county$County)

tmp <- create.case.test(level="county", county,countylist[1])
county.case.test.df <- data.frame(County=rep(countylist[1], nrow(tmp)), tmp)

for(i in 2:length(countylist)) {
  tmp <- create.case.test(level="county", county,countylist[i])
  county.case.test.df <- rbind(county.case.test.df, data.frame(County=rep(countylist[i], nrow(tmp)), tmp))
}

# assess missing vals
missing_vals <- sum(is.na(county.case.test.df$cases_percentdiff)) + sum(is.na(county.case.test.df$tests_percentdiff))
recorded_vals <- sum(!is.na(county.case.test.df$cases_percentdiff)) + sum(!is.na(county.case.test.df$tests_percentdiff))

# 11.88% w/ 2020-05-01
# 12.71% w/ 2020-04-22
# 12% w/ 2020-05-15
missing_vals / (missing_vals + recorded_vals)

write.csv(county.case.test.df, 'statistical-output/standard-stats/pct-change/county_pct_change.csv',
          row.names = FALSE)
```

### grouping

```{r}
colnames(county.case.test.df)[1] <- 'Level'
colnames(metro.case.test.df)[1] <- 'Level'
colnames(tsa.case.test.df)[1] <- 'Level'
colnames(phr.case.test.df)[1] <- 'Level'
state.case.test.df$Level <- 'Texas'

county.case.test.df$Level_Type = 'County'
metro.case.test.df$Level_Type = 'Metro'
tsa.case.test.df$Level_Type = 'TSA'
phr.case.test.df$Level_Type = 'PHR'
state.case.test.df$Level_Type = 'State'

combined.case.test.df <- rbind(county.case.test.df, metro.case.test.df,
                               tsa.case.test.df, state.case.test.df, phr.case.test.df)
write.csv(combined.case.test.df, 'statistical-output/standard-stats/pct-change/stacked_pct_change.csv',
          row.names = FALSE)

write.csv(combined.case.test.df, 'tableau/stacked_pct_change.csv',
          row.names = FALSE)
```


# STACK COMBINATIONS

```{r}
combined_current_ratio_dat$Date = max(combined.case.test.df$Date)
colnames(combined.arima.df.1)[5:6] = c('TS_CI_Lower', 'TS_CI_Upper')
colnames(combined.rt.df)[4:5] = c('RT_CI_Lower', 'RT_CI_Upper')

stacked_all = Reduce(function(x, y) merge(x, y, by = c('Level_Type', 'Level', 'Date'), all=TRUE),
       list(combined.case.test.df, combined_current_ratio_dat, 
            combined.arima.df.1, combined.rt.df, combined.hosp.arima.df.1))

write.csv(stacked_all, 'tableau/stacked_critical_trends.csv', row.names = FALSE)
```



## rolling average

Omitted fow now, Dr. Yaseen is processing this in Tableau

<!-- ### TSA -->

<!-- ```{r} -->
<!-- tsa<-read.csv("combined-datasets/tsa.csv") -->
<!-- tsa2 = tsa %>% group_by(TSA) %>% mutate(Cases_Cumulative_Mean_7Day = rollmean(Cases_Cumulative, 7, fill = NA), -->
<!--                                         Cases_Daily_Mean_7Day = rollmean(Cases_Daily, 7, fill = NA)) -->

<!-- write.csv(tsa2, 'combined-datasets/tsa.csv', row.names = F) -->
<!-- ``` -->

<!-- ### County -->

<!-- ```{r} -->
<!-- county<-read.csv("combined-datasets/county.csv") -->
<!-- county2 = county %>% group_by(County) %>% mutate(Cases_Cumulative_Mean_7Day = rollmean(Cases_Cumulative, 7, fill = NA), -->
<!--                                                  Cases_Daily_Mean_7Day = rollmean(Cases_Daily, 7, fill = NA)) -->

<!-- write.csv(county2, 'combined-datasets/county.csv', row.names = F) -->
<!-- ``` -->

<!-- ### Metro -->

<!-- ```{r} -->
<!-- metro <-read.csv("combined-datasets/metro.csv") -->

<!-- metro2 = metro %>% group_by(Metro_Area) %>% mutate(Cases_Cumulative_Mean_7Day = rollmean(Cases_Cumulative, 7, fill = NA), -->
<!--                                                    Cases_Daily_Mean_7Day = rollmean(Cases_Daily, 7, fill = NA)) -->

<!-- write.csv(metro2, 'combined-datasets/metro.csv', row.names = F) -->
<!-- ``` -->

<!-- ### State -->

<!-- ```{r} -->
<!-- state<-xlsx::read.xlsx('combined-datasets/state.xlsx', sheetIndex=1) -->

<!-- state2 = state %>% mutate(Cases_Cumulative_Mean_7Day = rollmean(Cases_Cumulative, 7, fill = NA), -->
<!--                           Cases_Daily_Mean_7Day = rollmean(Cases_Daily, 7, fill = NA)) -->

<!-- xlsx::write.xlsx(state2, file="combined-datasets/state.xlsx", sheetName="longitudinal", row.names=FALSE) -->
<!-- ``` -->



## Trends

Requires substantial cleaning & consideration for visualization - omitting until dashboard version 2

<!-- ```{r} -->
<!-- # read files -->
<!-- ## inflie dataset with county, date, case_daily and google mobility variables -->
<!-- county <- read.csv(file = 'combined-datasets/county.csv') -->
<!-- ## inflie dataset including agegroup and race -->
<!-- county_demo=read.csv(file = 'combined-datasets/county_demo.csv') -->
<!-- ##infile dateset with population by county -->
<!-- county_cases=read.csv(file ='original-sources/DSHS_county_cases.csv') -->
<!-- tsa = read.csv('original-sources/tsa_list.csv', header = F)[-1] -->
<!-- newcases.tsa=read.csv(file ='combined-datasets/tsa.csv') -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #combine files and prepare each variables -->
<!-- # dataset1 newcasesdata---county, date, case_daily and google mobility variables into newcasesdata -->
<!-- newcasesdata=subset(county[, c(1:2,4,11:16)]) #extract county, date, case_daily and google mobility variables into newcasesdata -->
<!-- newcasesdata_case=subset(newcasesdata[,c(1:3)])  #county,data,casedaily -->
<!-- newcasesdata_morbility=subset(newcasesdata[,c(1,4:9)]) #google morbility -->

<!-- #dataset 2 basic population variables by county from census -->
<!-- county_agerace=subset(county_demo[, c(1:3,8:9,52:53)]) #county, year, agegp, census, black male, black femal, hispanic male, hispanic female. This file was ordered by county, year, age group, so we need to sum up each variables by county as the whole census population for following computation -->
<!-- county_agerace$black=(county_agerace$BA_MALE+county_agerace$BA_FEMALE)##new variable- black_census=black_male+black_female -->
<!-- county_agerace$hispanic=(county_agerace$H_MALE+county_agerace$H_FEMALE) #new variable-hispanic_census=hispanic_male+hiapanic_female -->

<!--   #2.1  build age dataset -->
<!-- county_age_long=subset(county_agerace[,c(1:3)])    #select age group and population variable -->
<!-- county_age_wide=spread(county_age_long, AGEGRP,TOT_POP) #convert longdata to wide data of age_group, colname is age group,and rowname is county name -->

<!--   #2.2 build matrix for race information -->
<!-- county_race= matrix(nrow = nrow(county_age_wide),ncol = 4) -->
<!-- colnames(county_race)=c('County','Census','black_census','hispanic_census') -->
<!-- county_race=data.frame(county_race) -->
<!--   county_race$County=county_age_wide$County     #county variable -->
<!--   county_race$County=county_cases$County -->
<!--   county_race$Census=tapply(county_agerace[,3], county_agerace$County, FUN=sum) -->
<!--   county_race$black_census=tapply(county_agerace[,8], county_agerace$County, FUN=sum) -->
<!--   county_race$hispanic_census=tapply(county_agerace[,9], county_agerace$County, FUN=sum)    #this county_race dataset has variables county, census, black_census, hispanic_census -->
<!--   county_race$black_per=county_race$black_census/county_race$Census*100   #new variable black_per -->
<!--   county_race$hispanic_per=county_race$hispanic_census/county_race$Census*100   #new variable hispanic_per -->
<!--   #up to here, county_race has county,census,black_census, hispanic_census, black_per, hispanic_per -->

<!-- county_race_per=county_race[,c(1:2,5:6)]    #county_race has county, census, black_per, hispanic_per -->
<!-- county_modelvariables=merge(county_race_per, county_age_wide, by = 'County') #this is all the variables used for following modeling, including county, census, black_per, hispanic_per,age groups -->
<!-- county_newcases=merge(county_modelvariables,newcasesdata, by ='County')   #this is the final conbined dataset with all the county -->


<!-- #the following are prepared for dataset 3 -->
<!-- # TSA levels of model variables -->
<!-- tsa_long = reshape::melt(tsa, id = c('V2', 'V3')) -->
<!-- tsa_long_complete = subset(tsa_long, value != '')[, c(1, 2, 4)] -->
<!-- colnames(tsa_long_complete) = c('TSA', 'TSA_Name', 'County') -->
<!-- tsa_long_complete$County = trimws(tsa_long_complete$County) -->
<!-- tsa_merge= merge(county_modelvariables, tsa_long_complete, by = 'County', all = TRUE) -->
<!-- tsa_merge=tsa_merge[,c(1:23)] # merged dataset has county,census, black_per, hispanic_per, agegroups, tsa -->
<!-- tsa_merge$County=as.character(tsa_merge$County) -->
<!-- tsa_merge2=merge(county_race, tsa_long_complete, by = 'County', all = TRUE)   #county,census, black_census,hispanic_census, black_per,hispanic_per, agegroups tsa -->


<!-- #dataset 3  build tsa matrix including alll the varibales same as county_modelvariables, computated by tsa : county, census, black_per, hispanic_per,age groups -->
<!-- tsa_modelvariable=tsa_merge[,c(23,2:22)] -->
<!-- tsa_modelvariable= matrix(nrow = 22,ncol = 22) -->
<!-- tsa_modelvariable=data.frame(tsa_modelvariable) -->
<!--   colnames(tsa_modelvariable)=colnames(county_modelvariables) #county, census, black_per, hispanic_per,age groups -->
<!--   tsa_modelvariable$County =c("A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V")#county --tsa -->
<!--   tsa_modelvariable[,2]=tapply(tsa_merge[,2], tsa_merge$TSA, FUN=sum)    #census by tsa -->
<!--   tsa_modelvariable[,5]=tapply(tsa_merge[,5], tsa_merge$TSA, FUN=sum)    #5-22 are age groups by tsa -->
<!--   tsa_modelvariable[,6]=tapply(tsa_merge[,6], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,7]=tapply(tsa_merge[,7], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,8]=tapply(tsa_merge[,8], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,9]=tapply(tsa_merge[,9], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,10]=tapply(tsa_merge[,10], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,11]=tapply(tsa_merge[,11], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,12]=tapply(tsa_merge[,12], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,13]=tapply(tsa_merge[,13], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,14]=tapply(tsa_merge[,14], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,15]=tapply(tsa_merge[,15], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,16]=tapply(tsa_merge[,16], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,17]=tapply(tsa_merge[,17], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,18]=tapply(tsa_merge[,18], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,19]=tapply(tsa_merge[,19], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,20]=tapply(tsa_merge[,20], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,21]=tapply(tsa_merge[,21], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,22]=tapply(tsa_merge[,22], tsa_merge$TSA, FUN=sum) -->
<!--   tsa_modelvariable[,3]=tapply(tsa_merge2[,3], tsa_merge2$TSA, FUN=sum)  #black_census by tsa -->
<!--   tsa_modelvariable[,4]=tapply(tsa_merge2[,4], tsa_merge2$TSA, FUN=sum)  #hispanic_census by tsa -->
<!--   tsa_modelvariable[,3]=tsa_modelvariable[,3]/tsa_modelvariable$Census*100   #black_per -->
<!--   tsa_modelvariable[,4]=tsa_modelvariable[,4]/tsa_modelvariable$Census*100   #hispanic_per -->
<!-- tsa_modelvariable$County= paste0("tsa",tsa_modelvariable$County)  #change variables name , add tsa to each one -->


<!-- # dataset4 merge newcasesdata with tsa -->
<!-- tsa_newcasesdata=newcases.tsa[,c(2,1,5,12:17)] -->
<!-- colnames(tsa_newcasesdata)=colnames(newcasesdata)  #county, date, case_daily,retail_recreation, grocery_pharmacy, parks, transit, workplaces, residential, tsa -->
<!-- tsa_newcasesdata$County= as.character(tsa_newcasesdata$County) -->
<!-- tsa_newcasesdata$Date=as.Date(tsa_newcasesdata$Date) -->
<!-- tsa_newcasesdata$County= paste0("tsa",tsa_newcasesdata$County) -->

<!-- # dataset 5 combing all modelvaribles used for the following function , including county and tsa. both dataset have same column. -->
<!-- #           combing all newcasesdata used for the  following function , including county and tsa. both dataset have same column. -->
<!-- #model_variables=dplyr::bind_rows(county_modelvariables,tsa_modelvariable)   #final data1 -->
<!-- #newcasesdata$County=as.character(newcasesdata$County) -->
<!-- #newcasesdata$Date=as.Date(newcasesdata$Date) -->
<!-- #newcasesdata=dplyr::bind_rows(newcasesdata,tsa_newcasesdata)   #final data2 -->

<!-- ``` -->


<!-- ### county -->
<!-- ```{r} -->
<!-- # create data with 4-week (28 days) windows---county -->
<!-- # TODO: check "provided 9398 variables to replace 21 variables" -->
<!-- newcasesdata$Date=as.Date(newcasesdata$Date) -->
<!-- newcasesdata_case=newcasesdata[,c(1:3)] -->
<!-- newcasesdata_case_wide= spread(newcasesdata_case, County,Cases_Daily)    #convert long data to wide data of county -->
<!-- newcases=newcasesdata_case_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, cases_daily----county -->
<!-- newcases1=newcases[,-1] -->

<!-- datatrend.dat <- matrix(nrow=254*(nrow(newcases)-27), ncol=41) -->
<!-- datatrend.dat <- data.frame(datatrend.dat) -->
<!--   colnames(datatrend.dat) <- c(1:14, "ratio","MKteststat","MKpvalue","Spearteststat","Spearpvalue","Population","black_per","hispanic_per","age0_4","age10_14","age15_19","age20_24","age25_29","age30_34","age35_39","age40_44","age45_49","age5_9","age50_54","age55_59","age60_64","age65_69","age70_74","age75_79","age80_84","age85up","County") -->
<!--   datatrend.dat$County=rep(colnames(newcases1),nrow(newcases)-27) -->
<!-- #datatrend.dat$Population= ifelse(datatrend.dat$County == county_modelvariables$County, county_modelvariables$Census, NA) -->
<!--   datatrend.dat[,c(20:40)]=ifelse(datatrend.dat$County ==county_modelvariables$County, county_modelvariables[,c(2:22)], NA)  #value population,black_per, hispanic_per and each group by county -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # num.cycles is the number of 4-week windows within each TSA longitudinal data, after April 1, 2020. -->
<!-- num.cycles <- nrow(newcases)-27 -->
<!-- for(i in 1:num.cycles){ -->
<!--   for(j in 2:255){ -->
<!--     datatrend.dat[(i+(j-1)*num.cycles),1:14] <- newcases[i:(i+13), j] -->
<!--     # The "outcome" here is the ratio of total new cases -->
<!--     # in the following two weeks to the total new cases -->
<!--     # in the previous two weeks (percentage increase/decrease) -->
<!--     datatrend.dat[(i+(j-1)*num.cycles),15] <- sum(newcases[(i+14):(i+27),j])/sum(newcases[i:(i+13), j]) -->
<!--   } -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # remove data that seems wrong (ratio of cases is negative). These are from row 4, tsa D -->
<!-- datatrend.dat <- subset(datatrend.dat, ratio>0) -->
<!-- for(i in 1:nrow(datatrend.dat)){ -->
<!--   tmp <- MannKendall(datatrend.dat[i,1:14]) -->
<!--   datatrend.dat[i,16:17] <- c(tmp$tau, tmp$sl) -->
<!--   tmp <- cor.test(1:14, as.numeric(datatrend.dat[i,1:14]), method="spearman") -->
<!--   datatrend.dat[i,18:19] <- c(tmp$estimate, tmp$p.value) -->
<!-- } -->

<!-- #The following are to generate ratio2 to get bionary raio. ratio 3 to get three groups of ratio -->
<!-- datatrend.dat$ratio2=ifelse(datatrend.dat$ratio>1,1,0)       #ratio 2:ratio>1,1;ratio<=1,0 -->
<!-- datatrend.dat$ratio2=as.factor(datatrend.dat$ratio2)          # ratio2 : categorial -->

<!-- datatrend.dat$ratio3="" -->
<!-- for (i in 1:length(datatrend.dat$ratio)){ -->
<!-- if(datatrend.dat$ratio[i]<0.7){ -->
<!--   datatrend.dat$ratio3[i]=0 -->
<!-- } else if(datatrend.dat$ratio[i]>=0.7 & datatrend.dat$ratio[i]<1.3){ -->
<!--   datatrend.dat$ratio3[i]=1 -->
<!-- } else{ -->
<!--   datatrend.dat$ratio3[i]=2 -->
<!-- } -->
<!-- } -->
<!-- datatrend.dat$ratio3=as.factor(datatrend.dat$ratio3)    #ratio3: three groups of ratio; -->
<!-- ``` -->


<!-- ### tsa -->
<!-- ```{r} -->
<!-- # TODO: check "provided 814 variables to replace 21 variables" -->
<!-- # create data with 4-week (28 days) windows---county -->
<!-- tsa_newcasesdata$Date=as.Date(tsa_newcasesdata$Date) -->
<!-- tsa_newcasesdata_case=tsa_newcasesdata[,c(1:3)] -->
<!-- tsa_newcasesdata_case_wide= spread(tsa_newcasesdata_case, County,Cases_Daily)    #convert long data to wide data of tsa -->
<!-- tsa_newcases=tsa_newcasesdata_case_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, cases_daily----tsa -->
<!-- tsa_newcases1=tsa_newcases[,-1] -->

<!-- tsa_newcasesdata_retail=tsa_newcasesdata[,c(1:2,4)] -->
<!-- tsa_newcasesdata_retail_wide= spread(tsa_newcasesdata_retail, County,Retail_Recreation)    #convert long data to wide data of tsa -->
<!-- tsa_retail=tsa_newcasesdata_retail_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, Retail_Recreation----tsa -->

<!-- tsa_newcasesdata_grocery=tsa_newcasesdata[,c(1:2,5)] -->
<!-- tsa_newcasesdata_grocery_wide= spread(tsa_newcasesdata_grocery, County,Grocery_Pharmacy)    #convert long data to wide data of tsa -->
<!-- tsa_grocery=tsa_newcasesdata_grocery_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, Grocery_Pharmacy----tsa -->

<!-- tsa_newcasesdata_park=tsa_newcasesdata[,c(1:2,6)] -->
<!-- tsa_newcasesdata_park_wide= spread(tsa_newcasesdata_park, County,Parks)    #convert long data to wide data of tsa -->
<!-- tsa_park=tsa_newcasesdata_park_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, Grocery_Pharmacy----tsa -->

<!-- tsa_newcasesdata_transit=tsa_newcasesdata[,c(1:2,7)] -->
<!-- tsa_newcasesdata_transit_wide= spread(tsa_newcasesdata_transit, County, Transit)    #convert long data to wide data of tsa -->
<!-- tsa_transit=tsa_newcasesdata_transit_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, Grocery_Pharmacy----tsa -->

<!-- tsa_newcasesdata_workplace=tsa_newcasesdata[,c(1:2,8)] -->
<!-- tsa_newcasesdata_workplace_wide= spread(tsa_newcasesdata_workplace, County, Workplaces)    #convert long data to wide data of tsa -->
<!-- tsa_workplace=tsa_newcasesdata_workplace_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, rkplace------tsa -->

<!-- tsa_newcasesdata_Residential=tsa_newcasesdata[,c(1:2,9)] -->
<!-- tsa_newcasesdata_Residential_wide= spread(tsa_newcasesdata_Residential, County,Residential)    #convert long data to wide data of tsa -->
<!-- tsa_Residential=tsa_newcasesdata_Residential_wide %>% subset(Date>="2020-04-10")     #generate newcases matrix:county, date, Residential----tsa -->

<!-- tsa_datatrend.dat <- matrix(nrow=22*(nrow(tsa_newcases)-27), ncol=47) -->
<!-- tsa_datatrend.dat <- data.frame(tsa_datatrend.dat) -->
<!--   colnames(tsa_datatrend.dat) <- c(1:14, "ratio","MKteststat","MKpvalue","Spearteststat","Spearpvalue","Retail_Recreation","Grocery_Pharmacy","Parks","Transit","Workplaces","Residential","Population","black_per","hispanic_per","age0_4","age10_14","age15_19","age20_24","age25_29","age30_34","age35_39","age40_44","age45_49","age5_9","age50_54","age55_59","age60_64","age65_69","age70_74","age75_79","age80_84","age85up","County") -->
<!--   tsa_datatrend.dat$County=rep(colnames(tsa_newcases1),nrow(tsa_newcases)-27) -->
<!-- #datatrend.dat$Population= ifelse(datatrend.dat$County == county_modelvariables$County, county_modelvariables$Census, NA) -->
<!--   tsa_datatrend.dat[,c(26:46)]=ifelse(tsa_datatrend.dat$County ==tsa_modelvariable$County, tsa_modelvariable[,c(2:22)], NA)  #value, population,black_per, hispanic_per and each group by county -->
<!-- ``` -->


<!-- ```{r} -->
<!-- tsa_num.cycles <- nrow(tsa_newcases)-27 -->
<!-- for(i in 1:tsa_num.cycles){ -->
<!--   for(j in 2:23){ -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),1:14] <- tsa_newcases[i:(i+13), j] -->
<!--     # The "outcome" here is the ratio of total new cases -->
<!--     # in the following two weeks to the total new cases -->
<!--     # in the previous two weeks (percentage increase/decrease) -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),15] <- sum(tsa_newcases[(i+14):(i+27),j])/sum(tsa_newcases[i:(i+13), j]) -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),20] =rollmean(tsa_retail[i:(i+13), j], k=14, align="right") -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),21] =rollmean(tsa_grocery[i:(i+13), j], k=14, align="right") -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),22] =rollmean(tsa_park[i:(i+13), j], k=14, align="right") -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),23] =rollmean(tsa_transit[i:(i+13), j], k=14, align="right") -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),24] =rollmean(tsa_workplace[i:(i+13), j], k=14, align="right") -->
<!--     tsa_datatrend.dat[(i+(j-1)*tsa_num.cycles),25] =rollmean(tsa_Residential[i:(i+13), j], k=14, align="right") -->
<!--   } -->
<!-- } -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # remove data that seems wrong (ratio of cases is negative). These are from row 4, tsa D -->
<!-- #TODO: remove warnings and see if processing time is faster 'Cannot compute exact p-value with ties" -->
<!-- tsa_datatrend.dat <- subset(tsa_datatrend.dat, ratio>0) -->
<!-- for(i in 1:nrow(tsa_datatrend.dat)){ -->
<!--   tmp <- MannKendall(tsa_datatrend.dat[i,1:14]) -->
<!--   tsa_datatrend.dat[i,16:17] <- c(tmp$tau, tmp$sl) -->
<!--   tmp <- cor.test(1:14, as.numeric(tsa_datatrend.dat[i,1:14]), method="spearman") -->
<!--   tsa_datatrend.dat[i,18:19] <- c(tmp$estimate, tmp$p.value) -->
<!-- } -->

<!-- #The following are to generate ratio2 to get bionary raio. ratio 3 to get three groups of ratio -->
<!-- tsa_datatrend.dat$ratio2=ifelse(tsa_datatrend.dat$ratio>1,1,0)       #ratio 2:ratio>1,1;ratio<=1,0 -->
<!-- tsa_datatrend.dat$ratio2=as.factor(tsa_datatrend.dat$ratio2)          # ratio2 : categorial -->

<!-- tsa_datatrend.dat$ratio3="" -->
<!-- for (i in 1:length(tsa_datatrend.dat$ratio)){ -->
<!-- if(tsa_datatrend.dat$ratio[i]<0.7){ -->
<!--   tsa_datatrend.dat$ratio3[i]=0 -->
<!-- } else if(tsa_datatrend.dat$ratio[i]>=0.7 & tsa_datatrend.dat$ratio[i]<1.3){ -->
<!--   tsa_datatrend.dat$ratio3[i]=1 -->
<!-- } else{ -->
<!--   tsa_datatrend.dat$ratio3[i]=2 -->
<!-- } -->
<!-- } -->
<!-- tsa_datatrend.dat$ratio3=as.factor(tsa_datatrend.dat$ratio3)    #ratio3 three groups of ratio; -->
<!-- ``` -->


<!-- ### TODO: format dataframes and output -->


<!-- # MACHINE LEARNING -->
<!-- ## TODO: save output -->
<!-- ## County -->
<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- train= sample(1:dim(datatrend.dat)[1],dim(datatrend.dat)[1]/2) -->
<!-- test=-train -->
<!-- datatrend.county.train=datatrend.dat[train,]    #traiing set -->
<!-- datatrend.county.test=datatrend.dat[test,]     #testing set -->

<!-- #logistic -->
<!-- glm.county.fit=glm(ratio2 ~ MKteststat+ MKpvalue+ Spearteststat+ Spearpvalue +Population + black_per +hispanic_per + age0_4 + age10_14 +age15_19+age20_24+  age25_29 + age30_34 + age35_39 + age40_44 + age45_49  +  age5_9+  age50_54+  age55_59 + age60_64 + age65_69+ age70_74+ age75_79+ age80_84 +  age85up, data=datatrend.county.train,family=binomial)     #fit logistic model -->
<!-- summary(glm.county.fit)    #summany of logistics model -->
<!-- glm.county.probs=predict(glm.county.fit, datatrend.county.test, type="response")     #predict -->
<!-- glm.county.predict=rep("0",3399) -->
<!-- glm.county.predict[glm.county.probs > 0.5]="1"      #round-off -->
<!-- #glm.county.predict=as.factor(glm.county.predict) -->
<!-- table(glm.county.predict, datatrend.county.test$ratio2) -->
<!-- mean(glm.county.predict==datatrend.county.test$ratio2) -->

<!-- #random forest -->
<!-- #rf.county.train=randomForest(ratio ~ MKteststat+ MKpvalue+ Spearteststat+ Spearpvalue +Population + black_per +hispanic_per + age0_4 + age10_14 +age15_19+age20_24+  age25_29 + age30_34 + age35_39 + age40_44 + age45_49  +  age5_9+  age50_54+  age55_59 + age60_64 + age65_69+ age70_74+ age75_79+ age80_84 +  age85up, data=datatrend.county.train, mtry=5, importance =TRUE) -->
<!-- #yhat.rf.county = predict(rf.county.train, newdata=datatrend.dat.rf[-train ,]) -->
<!-- #mean((yhat.rf.county-datatrend.county.test)^2) -->
<!-- ``` -->

<!-- ## TSA -->
<!-- ```{r} -->
<!-- set.seed(1) -->
<!-- train= sample(1:dim(tsa_datatrend.dat)[1],dim(tsa_datatrend.dat)[1]/2) -->
<!-- test=-train -->
<!-- datatrend.tsa.train=tsa_datatrend.dat[train,]    #traiing set -->
<!-- datatrend.tsa.test=tsa_datatrend.dat[test,]     #testing set -->

<!-- #logistic -->
<!-- glm.tsa.fit=glm(ratio2 ~ MKteststat+ MKpvalue+ Spearteststat+ Spearpvalue +Population + black_per +hispanic_per + Retail_Recreation+ Grocery_Pharmacy   +  Parks  +  Transit +Workplaces+ Residential+age0_4 + age10_14 +age15_19+age20_24+  age25_29 + age30_34 + age35_39 + age40_44 + age45_49  +  age5_9+  age50_54+  age55_59 + age60_64 + age65_69+ age70_74+ age75_79+ age80_84 +  age85up, data=datatrend.tsa.train ,family=binomial)     #fit logistic model -->
<!-- summary(glm.tsa.fit)    #summany of logistics model -->
<!-- glm.tsa.probs=predict(glm.tsa.fit,datatrend.tsa.test,type="response")     #predict -->
<!-- glm.tsa.predict=rep("0",401) -->
<!-- glm.tsa.predict[glm.tsa.probs > 0.5]="1"      #round-off -->
<!-- #glm.county.predict=as.factor(glm.county.predict) -->
<!-- table(glm.tsa.predict, datatrend.tsa.test$ratio2) -->
<!-- mean(glm.tsa.predict==datatrend.tsa.test$ratio2) -->
<!-- ``` -->
